{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to pond_tools About pond_tools is a python package created to load and process data from the POND Network . This package is mainly used to merge official and unofficial POND exports, catergorize POND data into a multi-level column DataFrame sorted by measure, and combine measures together (e.g., IQ, Anxiety, Langage) Installation coming soon... Quick Start Guide coming coon...","title":"Getting Started"},{"location":"#welcome-to-pond_tools","text":"","title":"Welcome to pond_tools"},{"location":"#about","text":"pond_tools is a python package created to load and process data from the POND Network . This package is mainly used to merge official and unofficial POND exports, catergorize POND data into a multi-level column DataFrame sorted by measure, and combine measures together (e.g., IQ, Anxiety, Langage)","title":"About"},{"location":"#installation","text":"coming soon...","title":"Installation"},{"location":"#quick-start-guide","text":"coming coon...","title":"Quick Start Guide"},{"location":"core/","text":"Core combine_measures ( df , df_map , index_col = 'SUBJECT' , measure_prefix = 'CBCL' , sort_col = None , drop_duplicates = True ) Creates a new dataframe column that combines values from multiple old fields Parameters: Name Type Description Default df DataFrame DataFrame required df_map DataFrame DataFrame where index is the name of the new row and columns represent the different measures required Returns: Type Description DataFrame pd.DataFrame: DataFrame with new combined field added as column Source code in pond_tools\\core.py def combine_measures ( df : pd . DataFrame , df_map : pd . DataFrame , index_col : Optional [ str ] = \"SUBJECT\" , measure_prefix : Optional [ str ] = \"CBCL\" , sort_col : Optional [ str ] = None , drop_duplicates : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Creates a new dataframe column that combines values from multiple old fields Args: df (pd.DataFrame): DataFrame df_map: DataFrame where index is the name of the new row and columns represent the different measures Returns: pd.DataFrame: DataFrame with new combined field added as column \"\"\" df = df . copy () df_new = pd . DataFrame () for measure , fields in df_map . iteritems (): fields = fields . dropna () col_dict = df_map [ measure ] . dropna () . to_dict () col_dict = dict ( [( value , key ) for key , value in col_dict . items ()] ) # swap dict keys and items cols = [ index_col ] + [ i for i in df_map [ measure ] . dropna () . to_list () if i in df . columns ] temp = df [ cols ] . rename ( columns = col_dict ) drop_subset = temp . columns . to_list () drop_subset . remove ( index_col ) temp . insert ( 1 , f \" { measure_prefix } _MEASURE\" , measure ) df_new = df_new . append ( temp . dropna ( how = \"all\" , subset = drop_subset )) if sort_col == None : df_new = df_new if drop_duplicates : df_new = df_new . drop_duplicates ( subset = [ index_col ], keep = \"first\" ) else : df_new [ 'Non_Null_Count' ] = df_new . notnull () . sum ( axis = 1 ) df_new = df_new . sort_values ([ index_col , 'Non_Null_Count' , sort_col ]) if drop_duplicates : df_new = df_new . drop_duplicates ( subset = [ index_col ], keep = \"last\" ) df_new . drop ( columns = [ 'Non_Null_Count' ], inplace = True ) return df_new combine_measures_single ( df , new_field , old_fields , index_col = 'SUBJECT' ) Creates a new dataframe column that combines values from multiple old fields Parameters: Name Type Description Default df DataFrame DataFrame required new_field str Name of new field required old_fields List[str] List of old fields for combination in order of priority required Returns: Type Description DataFrame pd.DataFrame: DataFrame with new combined field added as column Source code in pond_tools\\core.py def combine_measures_single ( df : pd . DataFrame , new_field : str , old_fields : List [ str ], index_col : str = \"SUBJECT\" ) -> pd . DataFrame : \"\"\" Creates a new dataframe column that combines values from multiple old fields Args: df (pd.DataFrame): DataFrame new_field (str): Name of new field old_fields (List[str]): List of old fields for combination in order of priority Returns: pd.DataFrame: DataFrame with new combined field added as column \"\"\" df = df . copy () df_new = pd . DataFrame () for measure in old_fields : if measure in df . columns : temp = df [ index_col ] . to_frame () # temp['Measure'] = measure temp [ new_field ] = df [ measure ] df_new = df_new . append ( temp ) df_new = df_new . dropna () . drop_duplicates ( subset = [ index_col ], keep = \"first\" ) return df . merge ( df_new , how = \"left\" , left_index = True , right_index = True ) get_filepath ( filename ) Get filepath of the data/resources stored in the pond_tools module Parameters: Name Type Description Default filename str Name of filename required Returns: Type Description str str: Path of the file Source code in pond_tools\\core.py def get_filepath ( filename : str ) -> str : \"\"\" Get filepath of the data/resources stored in the pond_tools module Args: filename (str): Name of filename Returns: str: Path of the file \"\"\" return files ( \"pond_tools.resources\" ) . joinpath ( filename )","title":"Core"},{"location":"core/#core","text":"","title":"Core"},{"location":"core/#pond_tools.core.combine_measures","text":"Creates a new dataframe column that combines values from multiple old fields Parameters: Name Type Description Default df DataFrame DataFrame required df_map DataFrame DataFrame where index is the name of the new row and columns represent the different measures required Returns: Type Description DataFrame pd.DataFrame: DataFrame with new combined field added as column Source code in pond_tools\\core.py def combine_measures ( df : pd . DataFrame , df_map : pd . DataFrame , index_col : Optional [ str ] = \"SUBJECT\" , measure_prefix : Optional [ str ] = \"CBCL\" , sort_col : Optional [ str ] = None , drop_duplicates : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Creates a new dataframe column that combines values from multiple old fields Args: df (pd.DataFrame): DataFrame df_map: DataFrame where index is the name of the new row and columns represent the different measures Returns: pd.DataFrame: DataFrame with new combined field added as column \"\"\" df = df . copy () df_new = pd . DataFrame () for measure , fields in df_map . iteritems (): fields = fields . dropna () col_dict = df_map [ measure ] . dropna () . to_dict () col_dict = dict ( [( value , key ) for key , value in col_dict . items ()] ) # swap dict keys and items cols = [ index_col ] + [ i for i in df_map [ measure ] . dropna () . to_list () if i in df . columns ] temp = df [ cols ] . rename ( columns = col_dict ) drop_subset = temp . columns . to_list () drop_subset . remove ( index_col ) temp . insert ( 1 , f \" { measure_prefix } _MEASURE\" , measure ) df_new = df_new . append ( temp . dropna ( how = \"all\" , subset = drop_subset )) if sort_col == None : df_new = df_new if drop_duplicates : df_new = df_new . drop_duplicates ( subset = [ index_col ], keep = \"first\" ) else : df_new [ 'Non_Null_Count' ] = df_new . notnull () . sum ( axis = 1 ) df_new = df_new . sort_values ([ index_col , 'Non_Null_Count' , sort_col ]) if drop_duplicates : df_new = df_new . drop_duplicates ( subset = [ index_col ], keep = \"last\" ) df_new . drop ( columns = [ 'Non_Null_Count' ], inplace = True ) return df_new","title":"combine_measures()"},{"location":"core/#pond_tools.core.combine_measures_single","text":"Creates a new dataframe column that combines values from multiple old fields Parameters: Name Type Description Default df DataFrame DataFrame required new_field str Name of new field required old_fields List[str] List of old fields for combination in order of priority required Returns: Type Description DataFrame pd.DataFrame: DataFrame with new combined field added as column Source code in pond_tools\\core.py def combine_measures_single ( df : pd . DataFrame , new_field : str , old_fields : List [ str ], index_col : str = \"SUBJECT\" ) -> pd . DataFrame : \"\"\" Creates a new dataframe column that combines values from multiple old fields Args: df (pd.DataFrame): DataFrame new_field (str): Name of new field old_fields (List[str]): List of old fields for combination in order of priority Returns: pd.DataFrame: DataFrame with new combined field added as column \"\"\" df = df . copy () df_new = pd . DataFrame () for measure in old_fields : if measure in df . columns : temp = df [ index_col ] . to_frame () # temp['Measure'] = measure temp [ new_field ] = df [ measure ] df_new = df_new . append ( temp ) df_new = df_new . dropna () . drop_duplicates ( subset = [ index_col ], keep = \"first\" ) return df . merge ( df_new , how = \"left\" , left_index = True , right_index = True )","title":"combine_measures_single()"},{"location":"core/#pond_tools.core.get_filepath","text":"Get filepath of the data/resources stored in the pond_tools module Parameters: Name Type Description Default filename str Name of filename required Returns: Type Description str str: Path of the file Source code in pond_tools\\core.py def get_filepath ( filename : str ) -> str : \"\"\" Get filepath of the data/resources stored in the pond_tools module Args: filename (str): Name of filename Returns: str: Path of the file \"\"\" return files ( \"pond_tools.resources\" ) . joinpath ( filename )","title":"get_filepath()"},{"location":"documentation/","text":"Documentation pond_tools is comprised of three main modules core: general helper functions load: function to load, merge and subset the data measures: functions to process specific measures","title":"Documentation"},{"location":"documentation/#documentation","text":"pond_tools is comprised of three main modules core: general helper functions load: function to load, merge and subset the data measures: functions to process specific measures","title":"Documentation"},{"location":"examples/","text":"Examples","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"load/","text":"Load drop_duplicate_ids ( df , id_col = 'SUBJECT' , sort_col = None , ascending = True ) Drops duplicate ids by a set of criteria Parameters: Name Type Description Default df DataFrame DataFrame required id_col Optional[str] Name of ID column. Defaults to \"SUBJECT\". 'SUBJECT' sort_col Optional[str] Name of column used to sort values. Defaults to None. If None, it will look at date columns with differences and pick the oldest or newest value. None ascending Optional[bool] Option to sort in ascending order (or descending if False). Defaults to True. True Returns: Type Description DataFrame pd.DataFrame: DataFrame where duplicated IDs are dropped/ Source code in pond_tools\\load.py @pf . register_dataframe_method def drop_duplicate_ids ( df : pd . DataFrame , id_col : Optional [ str ] = \"SUBJECT\" , sort_col : Optional [ str ] = None , ascending : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Drops duplicate ids by a set of criteria Args: df (pd.DataFrame): DataFrame id_col (Optional[str], optional): Name of ID column. Defaults to \"SUBJECT\". sort_col (Optional[str], optional): Name of column used to sort values. Defaults to None. If None, it will look at date columns with differences and pick the oldest or newest value. ascending (Optional[bool], optional): Option to sort in ascending order (or descending if False). Defaults to True. Returns: pd.DataFrame: DataFrame where duplicated IDs are dropped/ \"\"\" df = df . copy () . drop_duplicates () vc = df [ id_col ] . value_counts () duplicate_subjects = ( vc [ vc > 1 ]) . index . to_list () if sort_col != None : return df . sort_values ( sort_col , ascending = ascending ) . drop_duplicates ( subset = [ id_col ], keep = 'first' ) else : indexes_to_drop = [] for subject in duplicate_subjects : df_subject = ( df . query ( f \" { id_col } == { subject } \" ) . get_duplicate_columns ( duplicate = False ) ) df_subject_dates = df_subject . search_columns ([ \"date\" , \"doc\" , \"instance\" ]) indexes_original = set ( df_subject . index . to_list ()) # If no different date columns, keep instance with the least nulls if df_subject_dates . shape [ 0 ] == 0 or df_subject_dates . shape [ 1 ] == 0 : keep = df_subject . notnull () . sum ( axis = 1 ) . sort_values () . index [ - 1 ] # If different date columns do exists, keep the earliest instance else : keep = df_subject_dates . sort_values ( df_subject_dates . columns [ 0 ], axis = 0 , ascending = ascending ) . index [ 0 ] indexes_to_drop . extend ( list ( indexes_original - set ([ keep ]))) df . drop ( index = indexes_to_drop ) return df . drop ( index = indexes_to_drop ) get_duplicate_ids ( df , id_col = 'SUBJECT' ) Returns dataframe of ID that are duplicated. Parameters: Name Type Description Default df DataFrame DataFrame required id_col Optional[str] Name of subject column. Defaults to 'SUBJECT'. 'SUBJECT' Returns: Type Description DataFrame pd.DataFrame: DataFrame with Duplicated IDs Source code in pond_tools\\load.py @pf . register_dataframe_method def get_duplicate_ids ( df : pd . DataFrame , id_col : Optional [ str ] = \"SUBJECT\" , ) -> pd . DataFrame : \"\"\" Returns dataframe of ID that are duplicated. Args: df (pd.DataFrame): DataFrame id_col (Optional[str], optional): Name of subject column. Defaults to 'SUBJECT'. Returns: pd.DataFrame: DataFrame with Duplicated IDs \"\"\" df = df . copy () vc = df [ id_col ] . value_counts () duplicate_subjects = ( vc [ vc > 1 ]) . index . to_list () duplicate_subjects return df . query ( f ' { id_col } .isin(@duplicate_subjects)' , engine = 'python' ) load_data ( filepath = WindowsPath ( 'C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/cache/POND_Merged_Data.pkl' ), measure_mappings = WindowsPath ( 'C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/mapping/MeasureMappings.xlsx' ), multicols = True , drop_9000 = True , drop_999 = True , iq = True , cbcl = True , anxiety = True , language = True , scq = True , rbs = True , demographics = True , geocode = False , cache = True , cache_loocation = WindowsPath ( 'C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/cache/LoadedCachedData.pkl' )) Reads a pond export or a pond export merged with additional files and returns a process DataFrame Parameters: Name Type Description Default filepath Optional[str] Filepath path of unprocessed/raw POND export. Defaults to MERGE_CACHE. WindowsPath('C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/cache/POND_Merged_Data.pkl') measure_mappings Optional[str] Measure mappings for multicolumn levels. Defaults to MEASURE_MAPPINGS. WindowsPath('C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/mapping/MeasureMappings.xlsx') multicols Optional[bool] Option to have multilevel columns determined by the measure_mappings . Defaults to True. True drop_9000 Optional[bool] Option to to filter out columns with values over 9000. Defaults to True. True drop_999 Optional[bool] Option to to filter out columns with values over 999. Defaults to True. True iq Optional[bool] Option to process IQ data. Defaults to True. True cbcl Optional[bool] Option to processes CBCL data. Defaults to True. True anxiety Optional[bool] Option to process Anxiety Data. Defaults to True. True language Optional[bool] Option to process Language Data. Defaults to True. True scq Optional[bool] Option to process SCQ Data. Defaults to True. True rbs Optional[bool] Option to process RBS data. Defaults to True. True demographics Optional[bool] Option to process Demographic Data. Defaults to True. True geocode Optional[bool] Option to process Geocode Data (note that this takes quite a bit of time +5 minutes). Defaults to False. False cache Optional[bool] Option to use cached data instead of re-running interprocessing script again. If `cache=False', the script will run again and the old cache will be overridden. Defaults to True. True cache_location Optional[str] Filepath of where to save and access cache. Defaults to LOAD_CACHE. required Returns: Type Description DataFrame pd.DataFrame: Processed DataFrame Source code in pond_tools\\load.py def load_data ( filepath : Optional [ str ] = MERGE_CACHE , measure_mappings : Optional [ str ] = MEASURE_MAPPINGS , multicols : Optional [ bool ] = True , drop_9000 : Optional [ bool ] = True , drop_999 : Optional [ bool ] = True , iq : Optional [ bool ] = True , cbcl : Optional [ bool ] = True , anxiety : Optional [ bool ] = True , language : Optional [ bool ] = True , scq : Optional [ bool ] = True , rbs : Optional [ bool ] = True , demographics : Optional [ bool ] = True , geocode : Optional [ bool ] = False , cache : Optional [ bool ] = True , cache_loocation : Optional [ str ] = LOAD_CACHE , ) -> pd . DataFrame : \"\"\" Reads a pond export or a pond export merged with additional files and returns a process DataFrame Args: filepath (Optional[str], optional): Filepath path of unprocessed/raw POND export. Defaults to MERGE_CACHE. measure_mappings (Optional[str], optional): Measure mappings for multicolumn levels. Defaults to MEASURE_MAPPINGS. multicols (Optional[bool], optional): Option to have multilevel columns determined by the `measure_mappings`. Defaults to True. drop_9000 (Optional[bool], optional): Option to to filter out columns with values over 9000. Defaults to True. drop_999 (Optional[bool], optional): Option to to filter out columns with values over 999. Defaults to True. iq (Optional[bool], optional): Option to process IQ data. Defaults to True. cbcl (Optional[bool], optional): Option to processes CBCL data. Defaults to True. anxiety (Optional[bool], optional): Option to process Anxiety Data. Defaults to True. language (Optional[bool], optional): Option to process Language Data. Defaults to True. scq (Optional[bool], optional): Option to process SCQ Data. Defaults to True. rbs (Optional[bool], optional): Option to process RBS data. Defaults to True. demographics (Optional[bool], optional): Option to process Demographic Data. Defaults to True. geocode (Optional[bool], optional): Option to process Geocode Data (note that this takes quite a bit of time +5 minutes). Defaults to False. cache (Optional[bool], optional): Option to use cached data instead of re-running interprocessing script again. If `cache=False', the script will run again and the old cache will be overridden. Defaults to True. cache_location (Optional[str], optional): Filepath of where to save and access cache. Defaults to LOAD_CACHE. Returns: pd.DataFrame: Processed DataFrame \"\"\" ########################################################## # Load data from cache ########################################################## try : if cache : return read_general ( cache_loocation ) except : print ( 'Cache does not exit. Set cache=False to create cache.' ) ########################################################## # Load data from file ########################################################## # Import pond data df = read_general ( filepath ) ########################################################## # Dropping unnecessary columns and filtering invalid values ########################################################## # Drop unnecessary columns if \"Unnamed: 0\" in df . columns : df = df . drop ( columns = [ \"Unnamed: 0\" ]) # Remove the non-numeric values if 'ANXIETYTOTAL' in df . columns : df [ \"ANXIETYTOTAL\" ] = df [ \"ANXIETYTOTAL\" ] . apply ( lambda s : _find_floats ( s )) df [ \"ANXIETYTOTAL\" ] = np . where ( df [ \"ANXIETYTOTAL\" ] > 900 , np . nan , df [ \"ANXIETYTOTAL\" ]) # Drop missing data code is 9000+ if drop_9000 : for col in DROP_9000 : df = df . filter_columns ( col = DROP_9000 , threshold = 9000 , comparison = '>' , replacement = np . nan ) if col in df . columns : old = df [ col ] df [ col ] = np . where ( old > 9000 , np . nan , old ) # Change string column to numeric if \"SA_TSCORE\" in df . columns : df [ \"SA_TSCORE\" ] = pd . to_numeric ( df [ \"SA_TSCORE\" ]) # Drop missing data code is 999+ if drop_999 : df = df . filter_columns ( col = DROP_999 , threshold = 999 , comparison = '>=' , replacement = np . nan ) # for col in DROP_999: # if col in df.columns: # old = df[col] # df[col] = np.where(old >= 999, np.nan, old) ########################################################## # Add extra measure-specific columns ########################################################## # IQ if iq : # Combine differend IQ measures df = combine_iq ( df ) # Calculate IQ gap df [ \"IQ_GAP\" ] = df [ \"PERF_IQ\" ] - df [ \"VERB_IQ\" ] # CBCL if cbcl : # Combine CBCL (0-6 and 6-18) df = combine_cbcl ( df ) # Anxiety Measures if anxiety : # Combine anxiety measures (Spence, RCADS) df = combine_anxiety ( df ) # Language if language : # Combine language measures (OWLS, PLS) df = combine_language ( df ) # SCQ if scq : # Calculate SCQ subdomains df = add_scq_subdomains ( df ) # RBS if rbs : # Calculate RBS-R Subscales df = add_rbs_subscales ( df ) # Demographics if demographics : # Add columns relating to co-occurring ASD, ADHD, OCD and TD Diagnoses df = add_diagnoses_columns ( df ) # Adds sum, max, min and mean columns to combine CAREGIVER 1 and 2 demographic data. df = add_combine_cargiver_columns ( df ) # Geocode if geocode : # Adds geocode data. Determines based on postal code what public health unit participant belongs in. Also merges census data. df = add_geocode_columns ( df ) ########################################################## # Rename columns ########################################################## # Rename sex/gender columns df . rename ( columns = { \"SEX\" : \"SEX_DEMO\" , \"NSI_SEX\" : \"SEX\" , \"SEX_STD\" : \"SEX_DEMO_STD\" , \"NSI_SEX_STD\" : \"SEX_STD\" , }, inplace = True , ) df . rename ( columns = ETHNICITY_MAPPING , inplace = True ) ########################################################## # Drop Duplicates ########################################################## # Drop Exact Duplicates df = df . drop_duplicates () # Drop duplicates from multiple RCADS test df = drop_duplicate_ids ( df ) ########################################################## # Create multi-level columns ########################################################## if multicols : # Get dataframe columns df_columns = ( df . columns . to_frame () . reset_index ( drop = True ) . rename ( columns = { 0 : \"Field\" }) ) # Get dictionary with category mappings measure_mapping_dict = ( read_general ( measure_mappings ) . melt ( var_name = \"Catergory\" , value_name = \"Field\" ) . dropna () . set_index ( \"Field\" ) . to_dict ()[ \"Catergory\" ] ) # Categorize fields and replace columns with multi-index df_columns [ \"Category\" ] = [ measure_mapping_dict [ i ] if i in measure_mapping_dict . keys () else \"None\" for i in df_columns [ \"Field\" ] ] df . columns = pd . MultiIndex . from_frame ( df_columns [[ \"Category\" , \"Field\" ]]) # Set index to participant number df [ \"Index\" ] = df [ \"Participant_Data\" ][ \"SUBJECT\" ] df . set_index ( \"Index\" , inplace = True ) else : df [ \"Index\" ] = df [ \"SUBJECT\" ] df . set_index ( \"Index\" , inplace = True ) ########################################################## # Save to cache if running data ########################################################## # Save df as cache df . to_pickle ( cache_loocation ) return df load_subset ( subset = [ 'Participant_Data' , 'SCQ' , 'IQ_Summary' ], filepath = 'DATA_FILEPATH' , field_cat = 'FIELD_CAT' , drop_upper_level = True , cache = True ) Reads and loads a melted dataframe with specied columns. Parameters: Name Type Description Default filepath Optional[str] Filepath to excel file. 'DATA_FILEPATH' field_cat Optional[str] Excel file with categorization of fields. 'FIELD_CAT' Returns: Type Description DataFrame pd.DataFrame: Loaded DataFrame Source code in pond_tools\\load.py def load_subset ( subset : List [ str ] = [ \"Participant_Data\" , \"SCQ\" , \"IQ_Summary\" ], filepath : Optional [ str ] = \"DATA_FILEPATH\" , field_cat : Optional [ str ] = \"FIELD_CAT\" , drop_upper_level : Optional [ bool ] = True , cache : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Reads and loads a melted dataframe with specied columns. Args: filepath: Filepath to excel file. field_cat: Excel file with categorization of fields. Returns: pd.DataFrame: Loaded DataFrame \"\"\" df = load_data ( filepath , field_cat , cache = cache ) df = df [ subset ] df . columns = df . columns . droplevel ( \"Category\" ) return df merge_data ( pond_export , extra_files = [], merge_extras = True , save = False , save_location = WindowsPath ( 'C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/cache/POND_Merged_Data.pkl' ), convert_datetime = True , merge_col = 'Subject' ) Merge official pond export with extra files (typically unofficial exports) Parameters: Name Type Description Default pond_export str Filepath of the official POND export required extra_files List[str] List of filepath(s) of the extra files (e.g., unofficial POND exports you want to merge with) [] merge_extras Optional[bool] Option to merge extra files. Defaults to True. True save Optional[bool] Option to save file with the cache in pond_tool module. Defaults to False. False save_location Optional[str] Filepath to where you want to save merged file. Defaults to MERGE_CACHE. WindowsPath('C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/cache/POND_Merged_Data.pkl') convert_datetime Optional[bool] Option to convert columns to datetime type. Defaults to True. True merge_col Optional[str] Name of index/subject column that your extra files are merging to. Defaults to 'Subject'. 'Subject' Returns: Type Description DataFrame pd.DataFrame: Merged DataFrame Source code in pond_tools\\load.py def merge_data ( pond_export : str , extra_files : List [ str ] = [], merge_extras : Optional [ bool ] = True , save : Optional [ bool ] = False , save_location : Optional [ str ] = MERGE_CACHE , convert_datetime : Optional [ bool ] = True , merge_col : Optional [ str ] = \"Subject\" , ) -> pd . DataFrame : \"\"\" Merge official pond export with extra files (typically unofficial exports) Args: pond_export (str): Filepath of the official POND export extra_files (List[str]): List of filepath(s) of the extra files (e.g., unofficial POND exports you want to merge with) merge_extras (Optional[bool], optional): Option to merge extra files. Defaults to True. save (Optional[bool], optional): Option to save file with the cache in pond_tool module. Defaults to False. save_location (Optional[str], optional): Filepath to where you want to save merged file. Defaults to MERGE_CACHE. convert_datetime (Optional[bool], optional): Option to convert columns to datetime type. Defaults to True. merge_col (Optional[str], optional): Name of index/subject column that your extra files are merging to. Defaults to 'Subject'. Returns: pd.DataFrame: Merged DataFrame \"\"\" # Load pond export df = read_general ( pond_export ) # Merge extra files if merge_extras and len ( extra_files ) != 0 : if isinstance ( extra_files , str ): extra_files = [ extra_files ] for i , file in enumerate ( extra_files ): try : df_file = pd . read_csv ( file ) except : df_file = pd . read_excel ( file ) df = df . merge ( df_file , how = \"left\" , left_on = \"SUBJECT\" , right_on = merge_col , suffixes = [ \"\" , f \"_ { i } \" ], ) # Convert datetime columns if convert_datetime : df = convert_datetime_cols ( df ) # Save file in cache if save : # tdate = datetime.today().strftime(\"%Y-%m-%d\") # df.to_pickle(get_filepath(fr\"cache/{tdate}-POND_Merged_Data.pkl\")) df . to_pickle ( save_location ) return df print_duplicate_ids ( df , id_col = 'SUBJECT' , return_indexes = True ) Prints the Subject with duplicates IDs along with dates or instances that are different Parameters: Name Type Description Default df DataFrame DataFrame required id_col Optional[str] Name of subject column. Defaults to 'SUBJECT'. 'SUBJECT' return_indexes Optional[bool] Return indexes of duplicated ids/subjects True Source code in pond_tools\\load.py @pf . register_dataframe_method def print_duplicate_ids ( df : pd . DataFrame , id_col : Optional [ str ] = \"SUBJECT\" , return_indexes : Optional [ bool ] = True ) -> None : \"\"\" Prints the Subject with duplicates IDs along with dates or instances that are different Args: df (pd.DataFrame): DataFrame id_col (Optional[str], optional): Name of subject column. Defaults to 'SUBJECT'. return_indexes (Optional[bool], optional): Return indexes of duplicated ids/subjects \"\"\" df = df . copy () vc = df [ id_col ] . value_counts () duplicate_subjects = ( vc [ vc > 1 ]) . index . to_list () for subject in duplicate_subjects : df_subject = df . query ( f \" { id_col } == { subject } \" ) . get_duplicate_columns ( duplicate = False ) print ( subject ) print ( df_subject . search_columns ([ \"date\" , \"doc\" , \"instance\" ])) print ( \" \\n \" ) if return_indexes : return duplicate_subjects","title":"Load"},{"location":"load/#load","text":"","title":"Load"},{"location":"load/#pond_tools.load.drop_duplicate_ids","text":"Drops duplicate ids by a set of criteria Parameters: Name Type Description Default df DataFrame DataFrame required id_col Optional[str] Name of ID column. Defaults to \"SUBJECT\". 'SUBJECT' sort_col Optional[str] Name of column used to sort values. Defaults to None. If None, it will look at date columns with differences and pick the oldest or newest value. None ascending Optional[bool] Option to sort in ascending order (or descending if False). Defaults to True. True Returns: Type Description DataFrame pd.DataFrame: DataFrame where duplicated IDs are dropped/ Source code in pond_tools\\load.py @pf . register_dataframe_method def drop_duplicate_ids ( df : pd . DataFrame , id_col : Optional [ str ] = \"SUBJECT\" , sort_col : Optional [ str ] = None , ascending : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Drops duplicate ids by a set of criteria Args: df (pd.DataFrame): DataFrame id_col (Optional[str], optional): Name of ID column. Defaults to \"SUBJECT\". sort_col (Optional[str], optional): Name of column used to sort values. Defaults to None. If None, it will look at date columns with differences and pick the oldest or newest value. ascending (Optional[bool], optional): Option to sort in ascending order (or descending if False). Defaults to True. Returns: pd.DataFrame: DataFrame where duplicated IDs are dropped/ \"\"\" df = df . copy () . drop_duplicates () vc = df [ id_col ] . value_counts () duplicate_subjects = ( vc [ vc > 1 ]) . index . to_list () if sort_col != None : return df . sort_values ( sort_col , ascending = ascending ) . drop_duplicates ( subset = [ id_col ], keep = 'first' ) else : indexes_to_drop = [] for subject in duplicate_subjects : df_subject = ( df . query ( f \" { id_col } == { subject } \" ) . get_duplicate_columns ( duplicate = False ) ) df_subject_dates = df_subject . search_columns ([ \"date\" , \"doc\" , \"instance\" ]) indexes_original = set ( df_subject . index . to_list ()) # If no different date columns, keep instance with the least nulls if df_subject_dates . shape [ 0 ] == 0 or df_subject_dates . shape [ 1 ] == 0 : keep = df_subject . notnull () . sum ( axis = 1 ) . sort_values () . index [ - 1 ] # If different date columns do exists, keep the earliest instance else : keep = df_subject_dates . sort_values ( df_subject_dates . columns [ 0 ], axis = 0 , ascending = ascending ) . index [ 0 ] indexes_to_drop . extend ( list ( indexes_original - set ([ keep ]))) df . drop ( index = indexes_to_drop ) return df . drop ( index = indexes_to_drop )","title":"drop_duplicate_ids()"},{"location":"load/#pond_tools.load.get_duplicate_ids","text":"Returns dataframe of ID that are duplicated. Parameters: Name Type Description Default df DataFrame DataFrame required id_col Optional[str] Name of subject column. Defaults to 'SUBJECT'. 'SUBJECT' Returns: Type Description DataFrame pd.DataFrame: DataFrame with Duplicated IDs Source code in pond_tools\\load.py @pf . register_dataframe_method def get_duplicate_ids ( df : pd . DataFrame , id_col : Optional [ str ] = \"SUBJECT\" , ) -> pd . DataFrame : \"\"\" Returns dataframe of ID that are duplicated. Args: df (pd.DataFrame): DataFrame id_col (Optional[str], optional): Name of subject column. Defaults to 'SUBJECT'. Returns: pd.DataFrame: DataFrame with Duplicated IDs \"\"\" df = df . copy () vc = df [ id_col ] . value_counts () duplicate_subjects = ( vc [ vc > 1 ]) . index . to_list () duplicate_subjects return df . query ( f ' { id_col } .isin(@duplicate_subjects)' , engine = 'python' )","title":"get_duplicate_ids()"},{"location":"load/#pond_tools.load.load_data","text":"Reads a pond export or a pond export merged with additional files and returns a process DataFrame Parameters: Name Type Description Default filepath Optional[str] Filepath path of unprocessed/raw POND export. Defaults to MERGE_CACHE. WindowsPath('C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/cache/POND_Merged_Data.pkl') measure_mappings Optional[str] Measure mappings for multicolumn levels. Defaults to MEASURE_MAPPINGS. WindowsPath('C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/mapping/MeasureMappings.xlsx') multicols Optional[bool] Option to have multilevel columns determined by the measure_mappings . Defaults to True. True drop_9000 Optional[bool] Option to to filter out columns with values over 9000. Defaults to True. True drop_999 Optional[bool] Option to to filter out columns with values over 999. Defaults to True. True iq Optional[bool] Option to process IQ data. Defaults to True. True cbcl Optional[bool] Option to processes CBCL data. Defaults to True. True anxiety Optional[bool] Option to process Anxiety Data. Defaults to True. True language Optional[bool] Option to process Language Data. Defaults to True. True scq Optional[bool] Option to process SCQ Data. Defaults to True. True rbs Optional[bool] Option to process RBS data. Defaults to True. True demographics Optional[bool] Option to process Demographic Data. Defaults to True. True geocode Optional[bool] Option to process Geocode Data (note that this takes quite a bit of time +5 minutes). Defaults to False. False cache Optional[bool] Option to use cached data instead of re-running interprocessing script again. If `cache=False', the script will run again and the old cache will be overridden. Defaults to True. True cache_location Optional[str] Filepath of where to save and access cache. Defaults to LOAD_CACHE. required Returns: Type Description DataFrame pd.DataFrame: Processed DataFrame Source code in pond_tools\\load.py def load_data ( filepath : Optional [ str ] = MERGE_CACHE , measure_mappings : Optional [ str ] = MEASURE_MAPPINGS , multicols : Optional [ bool ] = True , drop_9000 : Optional [ bool ] = True , drop_999 : Optional [ bool ] = True , iq : Optional [ bool ] = True , cbcl : Optional [ bool ] = True , anxiety : Optional [ bool ] = True , language : Optional [ bool ] = True , scq : Optional [ bool ] = True , rbs : Optional [ bool ] = True , demographics : Optional [ bool ] = True , geocode : Optional [ bool ] = False , cache : Optional [ bool ] = True , cache_loocation : Optional [ str ] = LOAD_CACHE , ) -> pd . DataFrame : \"\"\" Reads a pond export or a pond export merged with additional files and returns a process DataFrame Args: filepath (Optional[str], optional): Filepath path of unprocessed/raw POND export. Defaults to MERGE_CACHE. measure_mappings (Optional[str], optional): Measure mappings for multicolumn levels. Defaults to MEASURE_MAPPINGS. multicols (Optional[bool], optional): Option to have multilevel columns determined by the `measure_mappings`. Defaults to True. drop_9000 (Optional[bool], optional): Option to to filter out columns with values over 9000. Defaults to True. drop_999 (Optional[bool], optional): Option to to filter out columns with values over 999. Defaults to True. iq (Optional[bool], optional): Option to process IQ data. Defaults to True. cbcl (Optional[bool], optional): Option to processes CBCL data. Defaults to True. anxiety (Optional[bool], optional): Option to process Anxiety Data. Defaults to True. language (Optional[bool], optional): Option to process Language Data. Defaults to True. scq (Optional[bool], optional): Option to process SCQ Data. Defaults to True. rbs (Optional[bool], optional): Option to process RBS data. Defaults to True. demographics (Optional[bool], optional): Option to process Demographic Data. Defaults to True. geocode (Optional[bool], optional): Option to process Geocode Data (note that this takes quite a bit of time +5 minutes). Defaults to False. cache (Optional[bool], optional): Option to use cached data instead of re-running interprocessing script again. If `cache=False', the script will run again and the old cache will be overridden. Defaults to True. cache_location (Optional[str], optional): Filepath of where to save and access cache. Defaults to LOAD_CACHE. Returns: pd.DataFrame: Processed DataFrame \"\"\" ########################################################## # Load data from cache ########################################################## try : if cache : return read_general ( cache_loocation ) except : print ( 'Cache does not exit. Set cache=False to create cache.' ) ########################################################## # Load data from file ########################################################## # Import pond data df = read_general ( filepath ) ########################################################## # Dropping unnecessary columns and filtering invalid values ########################################################## # Drop unnecessary columns if \"Unnamed: 0\" in df . columns : df = df . drop ( columns = [ \"Unnamed: 0\" ]) # Remove the non-numeric values if 'ANXIETYTOTAL' in df . columns : df [ \"ANXIETYTOTAL\" ] = df [ \"ANXIETYTOTAL\" ] . apply ( lambda s : _find_floats ( s )) df [ \"ANXIETYTOTAL\" ] = np . where ( df [ \"ANXIETYTOTAL\" ] > 900 , np . nan , df [ \"ANXIETYTOTAL\" ]) # Drop missing data code is 9000+ if drop_9000 : for col in DROP_9000 : df = df . filter_columns ( col = DROP_9000 , threshold = 9000 , comparison = '>' , replacement = np . nan ) if col in df . columns : old = df [ col ] df [ col ] = np . where ( old > 9000 , np . nan , old ) # Change string column to numeric if \"SA_TSCORE\" in df . columns : df [ \"SA_TSCORE\" ] = pd . to_numeric ( df [ \"SA_TSCORE\" ]) # Drop missing data code is 999+ if drop_999 : df = df . filter_columns ( col = DROP_999 , threshold = 999 , comparison = '>=' , replacement = np . nan ) # for col in DROP_999: # if col in df.columns: # old = df[col] # df[col] = np.where(old >= 999, np.nan, old) ########################################################## # Add extra measure-specific columns ########################################################## # IQ if iq : # Combine differend IQ measures df = combine_iq ( df ) # Calculate IQ gap df [ \"IQ_GAP\" ] = df [ \"PERF_IQ\" ] - df [ \"VERB_IQ\" ] # CBCL if cbcl : # Combine CBCL (0-6 and 6-18) df = combine_cbcl ( df ) # Anxiety Measures if anxiety : # Combine anxiety measures (Spence, RCADS) df = combine_anxiety ( df ) # Language if language : # Combine language measures (OWLS, PLS) df = combine_language ( df ) # SCQ if scq : # Calculate SCQ subdomains df = add_scq_subdomains ( df ) # RBS if rbs : # Calculate RBS-R Subscales df = add_rbs_subscales ( df ) # Demographics if demographics : # Add columns relating to co-occurring ASD, ADHD, OCD and TD Diagnoses df = add_diagnoses_columns ( df ) # Adds sum, max, min and mean columns to combine CAREGIVER 1 and 2 demographic data. df = add_combine_cargiver_columns ( df ) # Geocode if geocode : # Adds geocode data. Determines based on postal code what public health unit participant belongs in. Also merges census data. df = add_geocode_columns ( df ) ########################################################## # Rename columns ########################################################## # Rename sex/gender columns df . rename ( columns = { \"SEX\" : \"SEX_DEMO\" , \"NSI_SEX\" : \"SEX\" , \"SEX_STD\" : \"SEX_DEMO_STD\" , \"NSI_SEX_STD\" : \"SEX_STD\" , }, inplace = True , ) df . rename ( columns = ETHNICITY_MAPPING , inplace = True ) ########################################################## # Drop Duplicates ########################################################## # Drop Exact Duplicates df = df . drop_duplicates () # Drop duplicates from multiple RCADS test df = drop_duplicate_ids ( df ) ########################################################## # Create multi-level columns ########################################################## if multicols : # Get dataframe columns df_columns = ( df . columns . to_frame () . reset_index ( drop = True ) . rename ( columns = { 0 : \"Field\" }) ) # Get dictionary with category mappings measure_mapping_dict = ( read_general ( measure_mappings ) . melt ( var_name = \"Catergory\" , value_name = \"Field\" ) . dropna () . set_index ( \"Field\" ) . to_dict ()[ \"Catergory\" ] ) # Categorize fields and replace columns with multi-index df_columns [ \"Category\" ] = [ measure_mapping_dict [ i ] if i in measure_mapping_dict . keys () else \"None\" for i in df_columns [ \"Field\" ] ] df . columns = pd . MultiIndex . from_frame ( df_columns [[ \"Category\" , \"Field\" ]]) # Set index to participant number df [ \"Index\" ] = df [ \"Participant_Data\" ][ \"SUBJECT\" ] df . set_index ( \"Index\" , inplace = True ) else : df [ \"Index\" ] = df [ \"SUBJECT\" ] df . set_index ( \"Index\" , inplace = True ) ########################################################## # Save to cache if running data ########################################################## # Save df as cache df . to_pickle ( cache_loocation ) return df","title":"load_data()"},{"location":"load/#pond_tools.load.load_subset","text":"Reads and loads a melted dataframe with specied columns. Parameters: Name Type Description Default filepath Optional[str] Filepath to excel file. 'DATA_FILEPATH' field_cat Optional[str] Excel file with categorization of fields. 'FIELD_CAT' Returns: Type Description DataFrame pd.DataFrame: Loaded DataFrame Source code in pond_tools\\load.py def load_subset ( subset : List [ str ] = [ \"Participant_Data\" , \"SCQ\" , \"IQ_Summary\" ], filepath : Optional [ str ] = \"DATA_FILEPATH\" , field_cat : Optional [ str ] = \"FIELD_CAT\" , drop_upper_level : Optional [ bool ] = True , cache : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Reads and loads a melted dataframe with specied columns. Args: filepath: Filepath to excel file. field_cat: Excel file with categorization of fields. Returns: pd.DataFrame: Loaded DataFrame \"\"\" df = load_data ( filepath , field_cat , cache = cache ) df = df [ subset ] df . columns = df . columns . droplevel ( \"Category\" ) return df","title":"load_subset()"},{"location":"load/#pond_tools.load.merge_data","text":"Merge official pond export with extra files (typically unofficial exports) Parameters: Name Type Description Default pond_export str Filepath of the official POND export required extra_files List[str] List of filepath(s) of the extra files (e.g., unofficial POND exports you want to merge with) [] merge_extras Optional[bool] Option to merge extra files. Defaults to True. True save Optional[bool] Option to save file with the cache in pond_tool module. Defaults to False. False save_location Optional[str] Filepath to where you want to save merged file. Defaults to MERGE_CACHE. WindowsPath('C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/cache/POND_Merged_Data.pkl') convert_datetime Optional[bool] Option to convert columns to datetime type. Defaults to True. True merge_col Optional[str] Name of index/subject column that your extra files are merging to. Defaults to 'Subject'. 'Subject' Returns: Type Description DataFrame pd.DataFrame: Merged DataFrame Source code in pond_tools\\load.py def merge_data ( pond_export : str , extra_files : List [ str ] = [], merge_extras : Optional [ bool ] = True , save : Optional [ bool ] = False , save_location : Optional [ str ] = MERGE_CACHE , convert_datetime : Optional [ bool ] = True , merge_col : Optional [ str ] = \"Subject\" , ) -> pd . DataFrame : \"\"\" Merge official pond export with extra files (typically unofficial exports) Args: pond_export (str): Filepath of the official POND export extra_files (List[str]): List of filepath(s) of the extra files (e.g., unofficial POND exports you want to merge with) merge_extras (Optional[bool], optional): Option to merge extra files. Defaults to True. save (Optional[bool], optional): Option to save file with the cache in pond_tool module. Defaults to False. save_location (Optional[str], optional): Filepath to where you want to save merged file. Defaults to MERGE_CACHE. convert_datetime (Optional[bool], optional): Option to convert columns to datetime type. Defaults to True. merge_col (Optional[str], optional): Name of index/subject column that your extra files are merging to. Defaults to 'Subject'. Returns: pd.DataFrame: Merged DataFrame \"\"\" # Load pond export df = read_general ( pond_export ) # Merge extra files if merge_extras and len ( extra_files ) != 0 : if isinstance ( extra_files , str ): extra_files = [ extra_files ] for i , file in enumerate ( extra_files ): try : df_file = pd . read_csv ( file ) except : df_file = pd . read_excel ( file ) df = df . merge ( df_file , how = \"left\" , left_on = \"SUBJECT\" , right_on = merge_col , suffixes = [ \"\" , f \"_ { i } \" ], ) # Convert datetime columns if convert_datetime : df = convert_datetime_cols ( df ) # Save file in cache if save : # tdate = datetime.today().strftime(\"%Y-%m-%d\") # df.to_pickle(get_filepath(fr\"cache/{tdate}-POND_Merged_Data.pkl\")) df . to_pickle ( save_location ) return df","title":"merge_data()"},{"location":"load/#pond_tools.load.print_duplicate_ids","text":"Prints the Subject with duplicates IDs along with dates or instances that are different Parameters: Name Type Description Default df DataFrame DataFrame required id_col Optional[str] Name of subject column. Defaults to 'SUBJECT'. 'SUBJECT' return_indexes Optional[bool] Return indexes of duplicated ids/subjects True Source code in pond_tools\\load.py @pf . register_dataframe_method def print_duplicate_ids ( df : pd . DataFrame , id_col : Optional [ str ] = \"SUBJECT\" , return_indexes : Optional [ bool ] = True ) -> None : \"\"\" Prints the Subject with duplicates IDs along with dates or instances that are different Args: df (pd.DataFrame): DataFrame id_col (Optional[str], optional): Name of subject column. Defaults to 'SUBJECT'. return_indexes (Optional[bool], optional): Return indexes of duplicated ids/subjects \"\"\" df = df . copy () vc = df [ id_col ] . value_counts () duplicate_subjects = ( vc [ vc > 1 ]) . index . to_list () for subject in duplicate_subjects : df_subject = df . query ( f \" { id_col } == { subject } \" ) . get_duplicate_columns ( duplicate = False ) print ( subject ) print ( df_subject . search_columns ([ \"date\" , \"doc\" , \"instance\" ])) print ( \" \\n \" ) if return_indexes : return duplicate_subjects","title":"print_duplicate_ids()"},{"location":"measures/","text":"Measures anxiety combine_anxiety ( df , drop_duplicates = True , merge = True ) Combines the subscales for Anxiety (0-6) and Anxiety (6-18) Parameters: Name Type Description Default df DataFrame DataFrame required drop_duplicates Optional[bool] Option to drop duplicates keeping the newest first. Defaults to True. True merge Optional[bool] Option to merge results to the original DataFrame. Defaults to True. True Returns: Type Description DataFrame pd.DataFrame: DataFrame Source code in pond_tools\\measures\\anxiety.py def combine_anxiety ( df : pd . DataFrame , drop_duplicates : Optional [ bool ] = True , merge : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Combines the subscales for Anxiety (0-6) and Anxiety (6-18) Args: df (pd.DataFrame): DataFrame drop_duplicates (Optional[bool], optional): Option to drop duplicates keeping the newest first. Defaults to True. merge (Optional[bool], optional): Option to merge results to the original DataFrame. Defaults to True. Returns: pd.DataFrame: DataFrame \"\"\" df_anxiety = combine_measures ( df = df , df_map = DF_ANXIETY_SUBSCALES , index_col = \"SUBJECT\" , measure_prefix = \"ANXIETY\" , sort_col = \"ANXIETY_DATE\" , drop_duplicates = drop_duplicates , ) if merge : df_anxiety . drop ( columns = [ \"SUBJECT\" ], inplace = True ) return df . merge ( df_anxiety , how = \"left\" , left_index = True , right_index = True ) else : return df_anxiety cbcl combine_cbcl ( df , drop_duplicates = True , merge = True ) Combines the subscales for CBCL (0-6) and CBCL (6-18) Parameters: Name Type Description Default df DataFrame DataFrame required drop_duplicates Optional[bool] Option to drop duplicates keeping the newest first. Defaults to True. True merge Optional[bool] Option to merge results to the original DataFrame. Defaults to True. True Returns: Type Description DataFrame pd.DataFrame: DataFrame Source code in pond_tools\\measures\\cbcl.py def combine_cbcl ( df : pd . DataFrame , drop_duplicates : Optional [ bool ] = True , merge : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Combines the subscales for CBCL (0-6) and CBCL (6-18) Args: df (pd.DataFrame): DataFrame drop_duplicates (Optional[bool], optional): Option to drop duplicates keeping the newest first. Defaults to True. merge (Optional[bool], optional): Option to merge results to the original DataFrame. Defaults to True. Returns: pd.DataFrame: DataFrame \"\"\" df_cbcl = combine_measures ( df = df , df_map = DF_CBCL_SUBSCALES , index_col = \"SUBJECT\" , measure_prefix = \"CBCL\" , sort_col = \"CBCL_DATE\" , drop_duplicates = drop_duplicates , ) if merge : df_cbcl . drop ( columns = [ \"SUBJECT\" ], inplace = True ) return df . merge ( df_cbcl , how = \"left\" , left_index = True , right_index = True ) else : return df_cbcl demographics add_combine_cargiver_columns ( df ) Adds sum, max, min and mean columns to combine CAREGIVER 1 and 2 demographic data. Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description DataFrame pd.DataFrame: DataFrame with combined caregiver columns Source code in pond_tools\\measures\\demographics.py def add_combine_cargiver_columns ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Adds sum, max, min and mean columns to combine CAREGIVER 1 and 2 demographic data. Args: df (pd.DataFrame): DataFrame Returns: pd.DataFrame: DataFrame with combined caregiver columns \"\"\" if set ( CAREGIVER_COLS ) . issubset ( df . columns ): df = df . copy () for prefix in CAREGIVER_PREFIXES : for stat , fn in COMBINATION_STATS_DICT . items (): cols = [ f ' { prefix } _1_STD' , f ' { prefix } _2_STD' ] df_filter = df . copy () . filter_columns ( col = cols , threshold = 9000 , comparison = '>' , replacement = np . nan ) df [ f ' { prefix } _ { stat } ' ] = df_filter [ cols ] . apply ( func = fn , axis = 1 ) return df else : print ( \"Missing columns required to calculate caregiver columns\" ) add_diagnoses_columns ( df ) Add columns relating to co-occurring ASD, ADHD, OCD and TD Diagnoses Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description DataFrame pd.DataFrame: DataFrame with diagnoses columns added Source code in pond_tools\\measures\\demographics.py def add_diagnoses_columns ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Add columns relating to co-occurring ASD, ADHD, OCD and TD Diagnoses Args: df (pd.DataFrame): DataFrame Returns: pd.DataFrame: DataFrame with diagnoses columns added \"\"\" if set ( CLINICAL_DX_COLS + [ 'PRIMARY_DIAGNOSIS' ]) . issubset ( df . columns ): df = df . copy () df [ \"ASD_DIAGNOSIS\" ] = df . apply ( lambda row : 1 if row [ \"PRIMARY_DIAGNOSIS\" ] == \"ASD\" or row [ \"CDCASDDX\" ] == 1 else 0 , axis = 1 , ) df [ \"ADHD_DIAGNOSIS\" ] = df . apply ( lambda row : 1 if row [ \"PRIMARY_DIAGNOSIS\" ] == \"ADHD\" or row [ \"CDCADHDX\" ] == 1 else 0 , axis = 1 , ) df [ \"OCD_DIAGNOSIS\" ] = df . apply ( lambda row : 1 if row [ \"PRIMARY_DIAGNOSIS\" ] == \"OCD\" or row [ \"CDCOCDDX\" ] == 1 else 0 , axis = 1 , ) df [ \"TD_DIAGNOSIS\" ] = df . apply ( lambda row : 1 if row [ \"PRIMARY_DIAGNOSIS\" ] == \"Typically Developing\" else 0 , axis = 1 , ) # Comorbid Diagnoses df [ \"COMORBID_DIAGNOSIS\" ] = np . nan df . loc [ df . query ( \"ASD_DIAGNOSIS == True & ADHD_DIAGNOSIS == True & OCD_DIAGNOSIS == True\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ASD_ADHD_OCD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == True & ADHD_DIAGNOSIS == True & OCD_DIAGNOSIS == False\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ASD_ADHD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == True & ADHD_DIAGNOSIS == False & OCD_DIAGNOSIS == True\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ASD_OCD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == False & ADHD_DIAGNOSIS == True & OCD_DIAGNOSIS == True\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ADHD_OCD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == True & ADHD_DIAGNOSIS == False & OCD_DIAGNOSIS == False\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ASD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == False & ADHD_DIAGNOSIS == True & OCD_DIAGNOSIS == False\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ADHD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == False & ADHD_DIAGNOSIS == False & OCD_DIAGNOSIS == True\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"OCD\" df . loc [ df . query ( \"TD_DIAGNOSIS == True\" ) . index , \"COMORBID_DIAGNOSIS\" ] = \"Typically Developing\" return df else : print ( \"Missing columns required to calculate diagnoses columns\" ) geocode add_geocode_columns ( df ) Adds geocode data. Determines based on postal code what public health unit participant belongs in. Also merges census data. Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description [type] [description] Source code in pond_tools\\measures\\geocode.py def add_geocode_columns ( df : pd . DataFrame ): \"\"\" Adds geocode data. Determines based on postal code what public health unit participant belongs in. Also merges census data. Args: df (pd.DataFrame): DataFrame Returns: [type]: [description] \"\"\" if 'POSTAL_CODE' in df . columns : df = df . copy () # Get location based data df [[ \"PLACE_NAME\" , \"LATITUDE\" , \"LONGITUDE\" ]] = df [ \"POSTAL_CODE\" ] . apply ( lambda x : get_geo_info ( x ) ) df [ \"PHU_NAME_E\" ] = df . get_geo_match ( geojson = ONTARIO_GEO , match_col = \"PHU_NAME_E\" , lat_col = \"LATITUDE\" , lng_col = \"LONGITUDE\" , ) df [ \"GEO_CODE\" ] = df [ \"PHU_NAME_E\" ] . replace ( PHU_MAPPING_DICT ) # Get census data df = ( df . reset_index () . merge ( get_census (), how = \"left\" , left_on = \"GEO_CODE\" , right_on = \"GEO_CODE\" ) . set_index ( \"index\" ) ) return df else : print ( \"Missing columns required to calculate geocode subdomains\" ) get_census ( filepath = WindowsPath ( 'C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/data/2016-Census-HealthRegion.csv' )) Reads census data and extracts the relevant columns Parameters: Name Type Description Default filepath str Filepath of census data. WindowsPath('C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/data/2016-Census-HealthRegion.csv') Returns: Type Description DataFrame pd.DataFrame: DataFrame with relevant census data Source code in pond_tools\\measures\\geocode.py def get_census ( filepath : str = CENSUS ) -> pd . DataFrame : \"\"\" Reads census data and extracts the relevant columns Args: filepath (str, optional): Filepath of census data. Returns: pd.DataFrame: DataFrame with relevant census data \"\"\" census = pd . read_csv ( filepath ) census_col = list ( CENSUS_COL_DICT . keys ()) census_col census_search = census . query ( f \"MEASURE.isin(@census_col)& GEO_LEVEL == 2 & GEO_CODE>=3000 & GEO_CODE <4000\" , engine = \"python\" , ) census_results = census_search . pivot ( index = \"GEO_CODE\" , columns = \"MEASURE\" , values = \"TOTAL_VALUE\" , ) . rename ( columns = CENSUS_COL_DICT ) census_results = census_results . astype ( \"float64\" ) return census_results iq combine_iq ( df , drop_duplicates = True , merge = True ) Combines the subscales for different IQ Measures Parameters: Name Type Description Default df DataFrame DataFrame required drop_duplicates Optional[bool] Option to drop duplicates keeping the newest first. Defaults to True. True merge Optional[bool] Option to merge results to the original DataFrame. Defaults to True. True Returns: Type Description DataFrame pd.DataFrame: DataFrame Source code in pond_tools\\measures\\iq.py def combine_iq ( df : pd . DataFrame , drop_duplicates : Optional [ bool ] = True , merge : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Combines the subscales for different IQ Measures Args: df (pd.DataFrame): DataFrame drop_duplicates (Optional[bool], optional): Option to drop duplicates keeping the newest first. Defaults to True. merge (Optional[bool], optional): Option to merge results to the original DataFrame. Defaults to True. Returns: pd.DataFrame: DataFrame \"\"\" df_iq = combine_measures ( df = df , df_map = DF_IQ_SUBSCALES , index_col = \"SUBJECT\" , measure_prefix = \"IQ\" , sort_col = \"IQ_DATE\" , drop_duplicates = False , ) df_iq [ 'Null_Count' ] = df_iq [[ 'FULL_IQ' ]] . isnull () . sum ( axis = 1 ) df_iq = df_iq . sort_values ([ 'SUBJECT' , 'Null_Count' , 'IQ_DATE' ,]) if drop_duplicates : # Keeps latest instance of IQ test with least non-null values df_iq = df_iq . drop_duplicates ( subset = [ 'SUBJECT' ], keep = 'first' ) df_iq . drop ( columns = [ 'Null_Count' ], inplace = True ) if merge : df_iq . drop ( columns = [ \"SUBJECT\" ], inplace = True ) return df . merge ( df_iq , how = \"left\" , left_index = True , right_index = True ) else : return df_iq language combine_language ( df , drop_duplicates = True , merge = True ) Combines the subscales for Language (OWLS, PLS) Parameters: Name Type Description Default df DataFrame DataFrame required drop_duplicates Optional[bool] Option to drop duplicates keeping the newest first. Defaults to True. True merge Optional[bool] Option to merge results to the original DataFrame. Defaults to True. True Returns: Type Description DataFrame pd.DataFrame: DataFrame Source code in pond_tools\\measures\\language.py def combine_language ( df : pd . DataFrame , drop_duplicates : Optional [ bool ] = True , merge : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Combines the subscales for Language (OWLS, PLS) Args: df (pd.DataFrame): DataFrame drop_duplicates (Optional[bool], optional): Option to drop duplicates keeping the newest first. Defaults to True. merge (Optional[bool], optional): Option to merge results to the original DataFrame. Defaults to True. Returns: pd.DataFrame: DataFrame \"\"\" df_language = combine_measures ( df = df , df_map = DF_LANGUAGE_SUBSCALES , index_col = \"SUBJECT\" , measure_prefix = \"LANGUAGE\" , sort_col = \"LANGUAGE_DATE\" , drop_duplicates = drop_duplicates , ) if merge : df_language . drop ( columns = [ \"SUBJECT\" ], inplace = True ) return df . merge ( df_language , how = \"left\" , left_index = True , right_index = True ) else : return df_language rbs add_rbs_subscales ( df ) Adds RBS subscales RSM(Repetitive Sensory Motor) and IS(Insistence on Sameness) from McDermott 2020 Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description DataFrame pd.DataFrame: DataFrame with RBS subscales added Source code in pond_tools\\measures\\rbs.py def add_rbs_subscales ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Adds RBS subscales RSM(Repetitive Sensory Motor) and IS(Insistence on Sameness) from McDermott 2020 Args: df (pd.DataFrame): DataFrame Returns: pd.DataFrame: DataFrame with RBS subscales added \"\"\" df = df . copy () if set ( f \"RBS { i } _STD\" for i in range ( 1 , 44 , 1 )) . issubset ( df . columns ): df [ \"RBS_RSM\" ] = df [ RBS_RSM ] . sum ( axis = 1 ) df [ \"RBS_IS\" ] = df [ RBS_IS ] . sum ( axis = 1 ) else : print ( \"Missing columns required to calculate SCQ subdomains\" ) return df scq add_scq_subdomains ( df ) Add subdomain columns, social, communication, rrb, and social communication subdomain columns to SCQ. The subdomains are not validated and based on mappings to the ADI-R subdomains. Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description DataFrame pd.DataFrame: DataFrame with SCQ Subdomain columns added Source code in pond_tools\\measures\\scq.py def add_scq_subdomains ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Add subdomain columns, social, communication, rrb, and social communication subdomain columns to SCQ. The subdomains are not validated and based on mappings to the ADI-R subdomains. Args: df (pd.DataFrame): DataFrame Returns: pd.DataFrame: DataFrame with SCQ Subdomain columns added \"\"\" df = df . copy () if set ( f \"SCQ { i } _STD\" for i in range ( 1 , 41 , 1 )) . issubset ( df . columns ): df [ \"SCQ_SOCIAL_DOMAIN\" ] = df [ SCQ_SOCIAL_DOMAIN ] . sum ( axis = 1 ) df [ \"SCQ_COMMUNICATION_DOMAIN\" ] = df [ SCQ_COMMUNICATION_DOMAIN ] . sum ( axis = 1 ) df [ \"SCQ_RRB_DOMAIN\" ] = df [ SCQ_RRB_DOMAIN ] . sum ( axis = 1 ) df [ \"SCQ_SC_DOMAIN\" ] = df [ SCQ_SC_DOMAIN ] . sum ( axis = 1 ) else : print ( \"Missing columns required to calculate SCQ subdomains\" ) return df","title":"Measures"},{"location":"measures/#measures","text":"","title":"Measures"},{"location":"measures/#pond_tools.measures.anxiety","text":"","title":"anxiety"},{"location":"measures/#pond_tools.measures.anxiety.combine_anxiety","text":"Combines the subscales for Anxiety (0-6) and Anxiety (6-18) Parameters: Name Type Description Default df DataFrame DataFrame required drop_duplicates Optional[bool] Option to drop duplicates keeping the newest first. Defaults to True. True merge Optional[bool] Option to merge results to the original DataFrame. Defaults to True. True Returns: Type Description DataFrame pd.DataFrame: DataFrame Source code in pond_tools\\measures\\anxiety.py def combine_anxiety ( df : pd . DataFrame , drop_duplicates : Optional [ bool ] = True , merge : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Combines the subscales for Anxiety (0-6) and Anxiety (6-18) Args: df (pd.DataFrame): DataFrame drop_duplicates (Optional[bool], optional): Option to drop duplicates keeping the newest first. Defaults to True. merge (Optional[bool], optional): Option to merge results to the original DataFrame. Defaults to True. Returns: pd.DataFrame: DataFrame \"\"\" df_anxiety = combine_measures ( df = df , df_map = DF_ANXIETY_SUBSCALES , index_col = \"SUBJECT\" , measure_prefix = \"ANXIETY\" , sort_col = \"ANXIETY_DATE\" , drop_duplicates = drop_duplicates , ) if merge : df_anxiety . drop ( columns = [ \"SUBJECT\" ], inplace = True ) return df . merge ( df_anxiety , how = \"left\" , left_index = True , right_index = True ) else : return df_anxiety","title":"combine_anxiety()"},{"location":"measures/#pond_tools.measures.cbcl","text":"","title":"cbcl"},{"location":"measures/#pond_tools.measures.cbcl.combine_cbcl","text":"Combines the subscales for CBCL (0-6) and CBCL (6-18) Parameters: Name Type Description Default df DataFrame DataFrame required drop_duplicates Optional[bool] Option to drop duplicates keeping the newest first. Defaults to True. True merge Optional[bool] Option to merge results to the original DataFrame. Defaults to True. True Returns: Type Description DataFrame pd.DataFrame: DataFrame Source code in pond_tools\\measures\\cbcl.py def combine_cbcl ( df : pd . DataFrame , drop_duplicates : Optional [ bool ] = True , merge : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Combines the subscales for CBCL (0-6) and CBCL (6-18) Args: df (pd.DataFrame): DataFrame drop_duplicates (Optional[bool], optional): Option to drop duplicates keeping the newest first. Defaults to True. merge (Optional[bool], optional): Option to merge results to the original DataFrame. Defaults to True. Returns: pd.DataFrame: DataFrame \"\"\" df_cbcl = combine_measures ( df = df , df_map = DF_CBCL_SUBSCALES , index_col = \"SUBJECT\" , measure_prefix = \"CBCL\" , sort_col = \"CBCL_DATE\" , drop_duplicates = drop_duplicates , ) if merge : df_cbcl . drop ( columns = [ \"SUBJECT\" ], inplace = True ) return df . merge ( df_cbcl , how = \"left\" , left_index = True , right_index = True ) else : return df_cbcl","title":"combine_cbcl()"},{"location":"measures/#pond_tools.measures.demographics","text":"","title":"demographics"},{"location":"measures/#pond_tools.measures.demographics.add_combine_cargiver_columns","text":"Adds sum, max, min and mean columns to combine CAREGIVER 1 and 2 demographic data. Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description DataFrame pd.DataFrame: DataFrame with combined caregiver columns Source code in pond_tools\\measures\\demographics.py def add_combine_cargiver_columns ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Adds sum, max, min and mean columns to combine CAREGIVER 1 and 2 demographic data. Args: df (pd.DataFrame): DataFrame Returns: pd.DataFrame: DataFrame with combined caregiver columns \"\"\" if set ( CAREGIVER_COLS ) . issubset ( df . columns ): df = df . copy () for prefix in CAREGIVER_PREFIXES : for stat , fn in COMBINATION_STATS_DICT . items (): cols = [ f ' { prefix } _1_STD' , f ' { prefix } _2_STD' ] df_filter = df . copy () . filter_columns ( col = cols , threshold = 9000 , comparison = '>' , replacement = np . nan ) df [ f ' { prefix } _ { stat } ' ] = df_filter [ cols ] . apply ( func = fn , axis = 1 ) return df else : print ( \"Missing columns required to calculate caregiver columns\" )","title":"add_combine_cargiver_columns()"},{"location":"measures/#pond_tools.measures.demographics.add_diagnoses_columns","text":"Add columns relating to co-occurring ASD, ADHD, OCD and TD Diagnoses Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description DataFrame pd.DataFrame: DataFrame with diagnoses columns added Source code in pond_tools\\measures\\demographics.py def add_diagnoses_columns ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Add columns relating to co-occurring ASD, ADHD, OCD and TD Diagnoses Args: df (pd.DataFrame): DataFrame Returns: pd.DataFrame: DataFrame with diagnoses columns added \"\"\" if set ( CLINICAL_DX_COLS + [ 'PRIMARY_DIAGNOSIS' ]) . issubset ( df . columns ): df = df . copy () df [ \"ASD_DIAGNOSIS\" ] = df . apply ( lambda row : 1 if row [ \"PRIMARY_DIAGNOSIS\" ] == \"ASD\" or row [ \"CDCASDDX\" ] == 1 else 0 , axis = 1 , ) df [ \"ADHD_DIAGNOSIS\" ] = df . apply ( lambda row : 1 if row [ \"PRIMARY_DIAGNOSIS\" ] == \"ADHD\" or row [ \"CDCADHDX\" ] == 1 else 0 , axis = 1 , ) df [ \"OCD_DIAGNOSIS\" ] = df . apply ( lambda row : 1 if row [ \"PRIMARY_DIAGNOSIS\" ] == \"OCD\" or row [ \"CDCOCDDX\" ] == 1 else 0 , axis = 1 , ) df [ \"TD_DIAGNOSIS\" ] = df . apply ( lambda row : 1 if row [ \"PRIMARY_DIAGNOSIS\" ] == \"Typically Developing\" else 0 , axis = 1 , ) # Comorbid Diagnoses df [ \"COMORBID_DIAGNOSIS\" ] = np . nan df . loc [ df . query ( \"ASD_DIAGNOSIS == True & ADHD_DIAGNOSIS == True & OCD_DIAGNOSIS == True\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ASD_ADHD_OCD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == True & ADHD_DIAGNOSIS == True & OCD_DIAGNOSIS == False\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ASD_ADHD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == True & ADHD_DIAGNOSIS == False & OCD_DIAGNOSIS == True\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ASD_OCD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == False & ADHD_DIAGNOSIS == True & OCD_DIAGNOSIS == True\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ADHD_OCD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == True & ADHD_DIAGNOSIS == False & OCD_DIAGNOSIS == False\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ASD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == False & ADHD_DIAGNOSIS == True & OCD_DIAGNOSIS == False\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"ADHD\" df . loc [ df . query ( \"ASD_DIAGNOSIS == False & ADHD_DIAGNOSIS == False & OCD_DIAGNOSIS == True\" ) . index , \"COMORBID_DIAGNOSIS\" , ] = \"OCD\" df . loc [ df . query ( \"TD_DIAGNOSIS == True\" ) . index , \"COMORBID_DIAGNOSIS\" ] = \"Typically Developing\" return df else : print ( \"Missing columns required to calculate diagnoses columns\" )","title":"add_diagnoses_columns()"},{"location":"measures/#pond_tools.measures.geocode","text":"","title":"geocode"},{"location":"measures/#pond_tools.measures.geocode.add_geocode_columns","text":"Adds geocode data. Determines based on postal code what public health unit participant belongs in. Also merges census data. Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description [type] [description] Source code in pond_tools\\measures\\geocode.py def add_geocode_columns ( df : pd . DataFrame ): \"\"\" Adds geocode data. Determines based on postal code what public health unit participant belongs in. Also merges census data. Args: df (pd.DataFrame): DataFrame Returns: [type]: [description] \"\"\" if 'POSTAL_CODE' in df . columns : df = df . copy () # Get location based data df [[ \"PLACE_NAME\" , \"LATITUDE\" , \"LONGITUDE\" ]] = df [ \"POSTAL_CODE\" ] . apply ( lambda x : get_geo_info ( x ) ) df [ \"PHU_NAME_E\" ] = df . get_geo_match ( geojson = ONTARIO_GEO , match_col = \"PHU_NAME_E\" , lat_col = \"LATITUDE\" , lng_col = \"LONGITUDE\" , ) df [ \"GEO_CODE\" ] = df [ \"PHU_NAME_E\" ] . replace ( PHU_MAPPING_DICT ) # Get census data df = ( df . reset_index () . merge ( get_census (), how = \"left\" , left_on = \"GEO_CODE\" , right_on = \"GEO_CODE\" ) . set_index ( \"index\" ) ) return df else : print ( \"Missing columns required to calculate geocode subdomains\" )","title":"add_geocode_columns()"},{"location":"measures/#pond_tools.measures.geocode.get_census","text":"Reads census data and extracts the relevant columns Parameters: Name Type Description Default filepath str Filepath of census data. WindowsPath('C:/Users/nguye/Documents/Thesis/pond_tools/pond_tools/resources/data/2016-Census-HealthRegion.csv') Returns: Type Description DataFrame pd.DataFrame: DataFrame with relevant census data Source code in pond_tools\\measures\\geocode.py def get_census ( filepath : str = CENSUS ) -> pd . DataFrame : \"\"\" Reads census data and extracts the relevant columns Args: filepath (str, optional): Filepath of census data. Returns: pd.DataFrame: DataFrame with relevant census data \"\"\" census = pd . read_csv ( filepath ) census_col = list ( CENSUS_COL_DICT . keys ()) census_col census_search = census . query ( f \"MEASURE.isin(@census_col)& GEO_LEVEL == 2 & GEO_CODE>=3000 & GEO_CODE <4000\" , engine = \"python\" , ) census_results = census_search . pivot ( index = \"GEO_CODE\" , columns = \"MEASURE\" , values = \"TOTAL_VALUE\" , ) . rename ( columns = CENSUS_COL_DICT ) census_results = census_results . astype ( \"float64\" ) return census_results","title":"get_census()"},{"location":"measures/#pond_tools.measures.iq","text":"","title":"iq"},{"location":"measures/#pond_tools.measures.iq.combine_iq","text":"Combines the subscales for different IQ Measures Parameters: Name Type Description Default df DataFrame DataFrame required drop_duplicates Optional[bool] Option to drop duplicates keeping the newest first. Defaults to True. True merge Optional[bool] Option to merge results to the original DataFrame. Defaults to True. True Returns: Type Description DataFrame pd.DataFrame: DataFrame Source code in pond_tools\\measures\\iq.py def combine_iq ( df : pd . DataFrame , drop_duplicates : Optional [ bool ] = True , merge : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Combines the subscales for different IQ Measures Args: df (pd.DataFrame): DataFrame drop_duplicates (Optional[bool], optional): Option to drop duplicates keeping the newest first. Defaults to True. merge (Optional[bool], optional): Option to merge results to the original DataFrame. Defaults to True. Returns: pd.DataFrame: DataFrame \"\"\" df_iq = combine_measures ( df = df , df_map = DF_IQ_SUBSCALES , index_col = \"SUBJECT\" , measure_prefix = \"IQ\" , sort_col = \"IQ_DATE\" , drop_duplicates = False , ) df_iq [ 'Null_Count' ] = df_iq [[ 'FULL_IQ' ]] . isnull () . sum ( axis = 1 ) df_iq = df_iq . sort_values ([ 'SUBJECT' , 'Null_Count' , 'IQ_DATE' ,]) if drop_duplicates : # Keeps latest instance of IQ test with least non-null values df_iq = df_iq . drop_duplicates ( subset = [ 'SUBJECT' ], keep = 'first' ) df_iq . drop ( columns = [ 'Null_Count' ], inplace = True ) if merge : df_iq . drop ( columns = [ \"SUBJECT\" ], inplace = True ) return df . merge ( df_iq , how = \"left\" , left_index = True , right_index = True ) else : return df_iq","title":"combine_iq()"},{"location":"measures/#pond_tools.measures.language","text":"","title":"language"},{"location":"measures/#pond_tools.measures.language.combine_language","text":"Combines the subscales for Language (OWLS, PLS) Parameters: Name Type Description Default df DataFrame DataFrame required drop_duplicates Optional[bool] Option to drop duplicates keeping the newest first. Defaults to True. True merge Optional[bool] Option to merge results to the original DataFrame. Defaults to True. True Returns: Type Description DataFrame pd.DataFrame: DataFrame Source code in pond_tools\\measures\\language.py def combine_language ( df : pd . DataFrame , drop_duplicates : Optional [ bool ] = True , merge : Optional [ bool ] = True , ) -> pd . DataFrame : \"\"\" Combines the subscales for Language (OWLS, PLS) Args: df (pd.DataFrame): DataFrame drop_duplicates (Optional[bool], optional): Option to drop duplicates keeping the newest first. Defaults to True. merge (Optional[bool], optional): Option to merge results to the original DataFrame. Defaults to True. Returns: pd.DataFrame: DataFrame \"\"\" df_language = combine_measures ( df = df , df_map = DF_LANGUAGE_SUBSCALES , index_col = \"SUBJECT\" , measure_prefix = \"LANGUAGE\" , sort_col = \"LANGUAGE_DATE\" , drop_duplicates = drop_duplicates , ) if merge : df_language . drop ( columns = [ \"SUBJECT\" ], inplace = True ) return df . merge ( df_language , how = \"left\" , left_index = True , right_index = True ) else : return df_language","title":"combine_language()"},{"location":"measures/#pond_tools.measures.rbs","text":"","title":"rbs"},{"location":"measures/#pond_tools.measures.rbs.add_rbs_subscales","text":"Adds RBS subscales RSM(Repetitive Sensory Motor) and IS(Insistence on Sameness) from McDermott 2020 Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description DataFrame pd.DataFrame: DataFrame with RBS subscales added Source code in pond_tools\\measures\\rbs.py def add_rbs_subscales ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Adds RBS subscales RSM(Repetitive Sensory Motor) and IS(Insistence on Sameness) from McDermott 2020 Args: df (pd.DataFrame): DataFrame Returns: pd.DataFrame: DataFrame with RBS subscales added \"\"\" df = df . copy () if set ( f \"RBS { i } _STD\" for i in range ( 1 , 44 , 1 )) . issubset ( df . columns ): df [ \"RBS_RSM\" ] = df [ RBS_RSM ] . sum ( axis = 1 ) df [ \"RBS_IS\" ] = df [ RBS_IS ] . sum ( axis = 1 ) else : print ( \"Missing columns required to calculate SCQ subdomains\" ) return df","title":"add_rbs_subscales()"},{"location":"measures/#pond_tools.measures.scq","text":"","title":"scq"},{"location":"measures/#pond_tools.measures.scq.add_scq_subdomains","text":"Add subdomain columns, social, communication, rrb, and social communication subdomain columns to SCQ. The subdomains are not validated and based on mappings to the ADI-R subdomains. Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description DataFrame pd.DataFrame: DataFrame with SCQ Subdomain columns added Source code in pond_tools\\measures\\scq.py def add_scq_subdomains ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Add subdomain columns, social, communication, rrb, and social communication subdomain columns to SCQ. The subdomains are not validated and based on mappings to the ADI-R subdomains. Args: df (pd.DataFrame): DataFrame Returns: pd.DataFrame: DataFrame with SCQ Subdomain columns added \"\"\" df = df . copy () if set ( f \"SCQ { i } _STD\" for i in range ( 1 , 41 , 1 )) . issubset ( df . columns ): df [ \"SCQ_SOCIAL_DOMAIN\" ] = df [ SCQ_SOCIAL_DOMAIN ] . sum ( axis = 1 ) df [ \"SCQ_COMMUNICATION_DOMAIN\" ] = df [ SCQ_COMMUNICATION_DOMAIN ] . sum ( axis = 1 ) df [ \"SCQ_RRB_DOMAIN\" ] = df [ SCQ_RRB_DOMAIN ] . sum ( axis = 1 ) df [ \"SCQ_SC_DOMAIN\" ] = df [ SCQ_SC_DOMAIN ] . sum ( axis = 1 ) else : print ( \"Missing columns required to calculate SCQ subdomains\" ) return df","title":"add_scq_subdomains()"}]}